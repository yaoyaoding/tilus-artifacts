# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# %%
import numpy as np
import pytest

import torch
import hidet
from hidet.ir import data_type
from hidet.ir.primitives.hip.mfma import MfmaConfig, mfma_sync
from hidet.ir.primitives.hip.buffer_addr import hip_buffer_load
from hidet.lang.hip import register_tensor, threadIdx
from hidet.lang import attrs

should_run = hidet.hip.available()
if should_run:
    capability = hidet.hip.capability()
    should_run = capability.gcnArchName == 'gfx90a'


# there is a constant that is architecture specific,
#     right now we only support gfx90a
@pytest.mark.skipif(not should_run, reason='hip is not available')
@pytest.mark.parametrize('dtype', ['float32', 'float16'])
@pytest.mark.parametrize('vec_load', [1, 2, 4, 8])
@pytest.mark.parametrize('n', [1, 2, 5, 17, 32, 33, 64, 65])
def test_buffer_load(dtype, vec_load, n):
    val_dtype = data_type(dtype)
    if val_dtype.nbytes * vec_load not in (1, 2, 4, 8, 16):
        return
    # if dtype == 'float32' and vec_load == 2:
    #     # for some reason, the code generated by this hangs the c++ compiler
    #     return

    nthreads = (n + vec_load - 1) // vec_load
    assert nthreads <= 1024
    with hidet.script_module() as module:

        @hidet.script
        def test_buffer_load(src: ~val_dtype, dst: ~val_dtype, n: data_type('int32')):
            attrs.hip.block_dim = nthreads
            attrs.hip.grid_dim = 1

            a = register_tensor(val_dtype, [vec_load])
            hip_buffer_load(src, n, threadIdx.x * vec_load, ~a[0], val_dtype, vec_load)

            for i in range(vec_load):
                if threadIdx.x * vec_load + i < n:
                    dst[threadIdx.x * vec_load + i] = a[i]

    ir_module = module.ir_module()
    cmodule = ir_module.build()

    torch_dtype = getattr(torch, dtype)
    a = torch.arange(0, n, dtype=torch_dtype, device='cuda')
    b = torch.empty([n], dtype=torch_dtype, device='cuda')

    cmodule(hidet.from_torch(a), hidet.from_torch(b), n)
    assert torch.allclose(a, b)


if __name__ == '__main__':
    pytest.main([__file__])
